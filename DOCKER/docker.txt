==================Docker==============
Problem & Why Docker
    Problem : Without Docker, you might run into several issues:
        Inconsistent Environments:
            Code might work on one machine but fail on another due to differences in libraries, dependencies, or system settings. Docker ensures consistency across different environments.
        Dependency Conflicts: 
            Managing dependencies manually can lead to conflicts. Different applications might require different versions of the same library, leading to incompatibility issues.
        Complex Setup: 
            Setting up environments manually can be time-consuming and error-prone. Docker simplifies this by encapsulating all dependencies within a container.
        Scalability Issues: 
            Scaling applications without Docker can be challenging. Docker allows you to run multiple containers easily, enabling horizontal scaling.
        Resource Inefficiency: 
            Running applications directly on the host system without isolation can lead to resource contention and inefficiency. Docker containers share the host system’s kernel, making them more resource-efficient.
        
        examples : 
            A new developer is joining a company needed to install requirements or dependence        Backend: 
                Backend : PHP,Laravel.PostgrsSQL,PostGIS,and Redis
                Frontend : JavaScript and Angular 4
                Installation Errors!
                Version Downgrade!
            Deploy Your app
                Testing, Staging, and production
        
    What / why Docker
        Docker is a platform that allows developers to package applications into containers1. Containers are lightweight, standalone packages that contain everything needed to run an application, including the code, runtime, libraries, and system settings2. Here are some key points about Docker and why it's useful: Consistency Across Environments: Docker containers run the same way on any environment, whether it's a developer's laptop, a test environment, or a production server. This consistency eliminates the "it works on my machine" problem
            Isolation: Containers are isolated from each other and the host system. This isolation ensures that applications run in a predictable manner without interfering with other applications.
            Resource Efficiency: Containers share the host system's kernel and are more lightweight than virtual machines, which require a full operating system. This makes them more resource-efficient.
            Portability: Docker containers can be easily moved between different environments, making it simple to deploy applications across various systems without compatibility issues.
            Scalability: Docker makes it easy to scale applications by running multiple containers on a single host or across multiple hosts.
            Development and Deployment: Docker simplifies the development, testing, and deployment process by providing a consistent environment for all stages of the software development lifecycle.

    create docker file (nodejs project)
        This file contains instructions to build a Docker image. For instance, it specifies the base image, environment setup, application dependencies, and the command to run the application.

        # Use the specified Node.js version
        FROM node:14 # Sets the base image to Node.js version 14

        # Set the working directory inside the container
        WORKDIR /app # Defines /app as the working directory

        # Copy package.json to the working directory
        COPY package.json . # Copies package.json to the working directory

        # Install dependencies from package.json
        RUN npm install # Runs npm install to install dependencies

        # Copy the rest of the project files to the working directory
        COPY . . # Copies all files from the current directory to the working directory in the container

        # Expose port 4000 for external communication
        EXPOSE 4000 # Specifies that the container listens on port 4000

        # Define the command to run the application
        CMD ["npm", "start"] # Runs npm start to start the application


    docker hub 
        Docker Hub is a cloud-based repository for storing, managing, and sharing container images.
        It's the world's largest container registry and simplifies the process of developing, distributing,
        and deploying containerized applications. Here are some key features:

            Image Repository: Docker Hub hosts a vast collection of container images that you can use as a base for your projects.

            Public and Private Repositories: You can create public repositories to share your images with the community or 
            private repositories to share with your team securely.

            Automated Builds: Docker Hub can automatically build images from a Dockerfile and push them to the repository.

            Collaboration: It allows teams to collaborate on container images, making it easier to manage and distribute applications.

            Pre-built Images: Docker Hub provides a wide range of pre-built images for popular software, which can speed 
            up your development process.
        
    Build the Docker Image:

        Use the Dockerfile to create a Docker image with the docker build command.
        docker build -t my-node-app . : Builds an image named my-node-app from the current directory.

    Run Docker Containers:

        Once you have the image, you can create multiple containers from it. Each container runs as an isolated instance of the application.

        docker run -d --name container1 -p 4001:4000 my-node-app : Runs the first container in detached mode, mapping port 4001 on the host to port 4000 in the container.
        docker run -d --name container2 -p 4002:4000 my-node-app : Runs the second container similarly.
    
    Docker Optimization
        Do I need to copy all files?
            No, you don’t need to copy all files. Using a .dockerignore file helps exclude unnecessary files and directories. 
            
            Here’s an example:
                plaintext
                Copy
                /node_modules
                Dockerfile
                .env

            This ensures that Docker does not include these files in the build context, speeding up the build process and reducing the image size.
    Why did we split the package.json copy command?
        Splitting the COPY command into two parts is a common optimization strategy. By copying package.json first and running npm install before copying the rest of the files, Docker can cache the layers more efficiently. Here’s why:

        If only package.json and package-lock.json have changed, Docker can reuse the cached layers for installing dependencies, making the build process faster.

        If we copy all files first and then run npm install, any change in your source files will cause Docker to rerun the npm install step, which can be time-consuming.
